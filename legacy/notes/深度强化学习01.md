# 深度强化学习 01

## policy gradient

基本组成：

    {policy,environment,reward function}

对于深度强化学习,policy就是一个参数为θ的神经网络，它接受environment 的观测输入，它的输出层每个神经元对应一个可能的行动

我们试图通过调整θ使得回报R最大。R并不是标量，它是随机变量，仅可以直到其期望

### 迭代方法

---

只要想办法求得R关于θ的微分，用梯度上升就能逐渐增大R

$$ \nabla \overline{R} = \sum_{\tau} R(\tau)\nabla p_\theta (\tau) $$

其中$\tau$ 是某一轮互动的数据， $p_\theta(\tau)$是执行某个策略的概率，$R(\tau)$是采取这个策略所得奖励

使用$\nabla f(x) = f(x) \nabla log f(x)$
可以提出一个$p_\theta$，
使得前面的式子变成

$$ \nabla \overline{R} = E_{\tau \sim p_\theta(\tau)} [R(\tau)\nabla log p_\theta(\tau)] $$

这样一来，就可以用样本均值来近似。在近似后的式子中$\nabla logp_\theta(\tau^n)$仅于智能体本身有关而与环境无关，最终这一项可以表示为$p_\theta(a_t^n|s_t^n)$，表示在状态s下采取行动a的概率，这样它就可计算了

$$ \nabla \overline{R_\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau ^ n)\nabla log p_\theta(a_t^n|s_t^n) $$

观察这个式子不难发现梯度上升方向的意义非常符合直觉：假如当前状态s下采取a得到了正的梯度，则这是好的，改进使得$p(a|s)$更大，反之a是坏的，改进会使得它变小

---

最终智能体的更新策略为

$$ \theta \gets \theta + \eta \nabla \overline{R_\theta} $$

其中
$$\theta：参数$$
$$\eta：学习速率$$

$$ \nabla \overline{R_\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau ^ n)\nabla log p_\theta(a_t^n|s_t^n) $$

---

### 模型构建

---

策略模型本质上就是一个分类器。

    input：环境，例如一个图像或者一个图像的序列
    output：动作

我们训练这个分类网络的数据源就来源于环境，而loss function就是上面的

$$ \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau ^ n)\nabla log p_\theta(a_t^n|s_t^n)  $$

比起一般的loss function，它多了一个权重（就是一场游戏的结算奖励）。用自动梯度可以求出

---

#### T1 add baseline

R不一定有正有负，需要一个baseline是的分数有正有负。

若不然，智能体会认为“任意一个action的概率都应该被增加，只不过是依据reward增加多少的问题”。在可以精确求得E时无所谓，但在sampling时未被sample到的action的概率会不断被稀释

可以减去一个R的期望近似值

此时梯度式变为

$$ \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau ^ n) - b)\nabla log p_\theta(a_t^n|s_t^n)  $$

---

#### T2 给每个action适当分值

上面的求梯度忽略对单个action是不公平的。R高不代表每个action都正确，R低不代表每个action都错误。这在sample较少时会引发问题

一个改进方法是试图计算每一步的贡献 = 总贡献-之前步数贡献的前缀和

则求梯度的式子改变为

$$ \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} \sum_{t' = t}^{T_n} ( r_{t'}^n - b )\nabla log p_\theta(a_t^n|s_t^n)  $$

其中r为上面所述的单步回报

#### T3 为先前的探索加discount factor

先前的的经验比起最近的不那么重要，因此加一个衰减因子$\gamma<1$使得其权重衰减

此时梯度式变为

$$ \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} \sum_{t' = t}^{T_n} ( \gamma ^{t' - t} r_{t'}^n - b )\nabla log p_\theta(a_t^n|s_t^n)  $$

总结起来，使用Advantage function $A^\theta(s_t,a_t)=\gamma^{t' - t}r_{t'}^n - b$表示采取某种行动的相较别的行动有多大好处

我们可以看见上面的种种修改实际上就是对advantage function的改进
