# On-policy → Off-policy

## 二者区别

直观来说，On-policy让智能体自己探索，Off-policy则是让智能体旁观

具体来说：

这是On-Policy求R梯度的式子

$$ \nabla \overline{R} = E_{\tau \sim p_\theta(\tau)} [R(\tau)\nabla log p_\theta(\tau)] $$

On-policy在求回报R的梯度时用到了在参数θ下，以轨道为随机变量的回报期望$E_{\tau \sim p_\theta(\tau)}$

p.s. τ，"轨道"
![avatar](/notepic/tau_defination.png)

On-policy求梯度的式子与轨道τ有关（$p_{\tau \sim p_\theta(\tau)}$），而轨道τ又与θ有关

这样的话每调一次参数就得重新让智能体取环境里润一遍来获得新的τ用于求这个期望

就是说在这浪费了之前的所有经验，稍微改下策略就得从头收集数据

Off-policy就是试图sample一次，update参数多次

## Importance Sampling

这个东西就是一种近似求期望的方法

### 基础式子

首先，在大多数情况下

$$E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{N}^{i=1} f(x_i)$$

其中$x_i$是$p(x)$分布中取得的一组样本

### 问题

如果$x_i$**不是**$p(x)$分布中取得的一组样本，上面求期望的近似就不成立了。

p.s.在本主题下，τ符合的分布就会随着θ的改变而改变，因此On-policy的期望不能复用

### 改进

从函数期望的式子本身开始思考

$$E_{x \sim p}[f(x)] = \int f(x)p(x)$$

而对于$x' \sim q$，有$\frac{p(x)}{q(x)} = p(x)$

因此可以取样x'来计算$E_x$，只要直到二者的概率密度函数就能完成x'→x的变换

$$E_{x \sim p}[f(x)] = \int f(x') \frac{p(x')}{q(x')} x'$$

### 实际

p、q不能差太远。这从直观上很好理解。用别的分布来求本来的数字特征是带有偏见的

尽管有

$$\frac{p(x)}{q(x)} = p(x)$$

但是
$x' \sim q$，有$\frac{p(x')}{q(x')} x' !\sim p$

从数字特征上可以观察方差$VAR[x] = E[X^2] - (E[X])^2$

$$Var_{x \sim p} = E_{x \sim p}[f(x)^2] - (E_{x \sim p}[f(x)])^2$$

$$Var_{x \sim p} = E_{x \sim p}[f(x)^2 \frac{p(x')}{q(x')}] - (E_{x \sim p}[f(x)])^2$$
 
它们不一样。变换前后就期望一样，方差差得太大还是会离谱的

积分没问题，取样问题就可能很大。取样的偏见很可能得出离谱的数字特征估计值。不过好在本话题下参数相近的网络行为差异也不会过大

## Off-Policy

利用 Important Sampling 改变计算梯度的式子

$$ \nabla \overline{R} = E_{\tau \sim p_{\theta'}(\tau)} [\frac{p_\theta(\tau)}{p_{\theta'}(\tau)}R(\tau)\nabla log p_\theta(\tau)] $$

一个τ可以用很多次，直到θ与θ'差距过大再重新采样